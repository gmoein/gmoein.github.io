{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Pytorch_Lenet.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"3ED3mPk_xSHU","colab_type":"text"},"source":["#densenet\n","#dpn\n","#efficientnet\n","#googlenet\n","#lenet\n","#mobilenet\n","#mobilenetv2\n","#pnasnet\n","#preact_resnet\n","#resnet\n","#resnext\n","#senet\n","#shufflenet\n","#shufflenetv2\n","#vgg"]},{"cell_type":"markdown","metadata":{"id":"bOiYxjGgx90N","colab_type":"text"},"source":["https://towardsdatascience.com/illustrated-10-cnn-architectures-95d78ace614d"]},{"cell_type":"code","metadata":{"id":"gIlHog4ytl3Y","colab_type":"code","colab":{}},"source":["'''LeNet in PyTorch.'''\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class LeNet(nn.Module):\n","    def __init__(self):\n","        super(LeNet, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 6, 5)\n","        self.conv2 = nn.Conv2d(6, 16, 5)\n","        self.fc1   = nn.Linear(16*5*5, 120)\n","        self.fc2   = nn.Linear(120, 84)\n","        self.fc3   = nn.Linear(84, 10)\n","\n","    def forward(self, x):\n","        out = F.relu(self.conv1(x))\n","        out = F.max_pool2d(out, 2)\n","        out = F.relu(self.conv2(out))\n","        out = F.max_pool2d(out, 2)\n","        out = out.view(out.size(0), -1)\n","        out = F.relu(self.fc1(out))\n","        out = F.relu(self.fc2(out))\n","        out = self.fc3(out)\n","        return out"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"B3_VII7mt3xJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"836d4b22-9456-434c-e10b-b1265d59a104","executionInfo":{"status":"ok","timestamp":1574839344498,"user_tz":-210,"elapsed":1199,"user":{"displayName":"Moein Ahmadi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCV90MQinzCmUJxaK3iqt_P5suRKsJgPyaFSIkW=s64","userId":"11078830524272987147"}}},"source":["'''DenseNet in PyTorch.'''\n","import math\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class Bottleneck(nn.Module):\n","    def __init__(self, in_planes, growth_rate):\n","        super(Bottleneck, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = nn.Conv2d(in_planes, 4*growth_rate, kernel_size=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(4*growth_rate)\n","        self.conv2 = nn.Conv2d(4*growth_rate, growth_rate, kernel_size=3, padding=1, bias=False)\n","\n","    def forward(self, x):\n","        out = self.conv1(F.relu(self.bn1(x)))\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out = torch.cat([out,x], 1)\n","        return out\n","\n","\n","class Transition(nn.Module):\n","    def __init__(self, in_planes, out_planes):\n","        super(Transition, self).__init__()\n","        self.bn = nn.BatchNorm2d(in_planes)\n","        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=1, bias=False)\n","\n","    def forward(self, x):\n","        out = self.conv(F.relu(self.bn(x)))\n","        out = F.avg_pool2d(out, 2)\n","        return out\n","\n","\n","class DenseNet(nn.Module):\n","    def __init__(self, block, nblocks, growth_rate=12, reduction=0.5, num_classes=10):\n","        super(DenseNet, self).__init__()\n","        self.growth_rate = growth_rate\n","\n","        num_planes = 2*growth_rate\n","        self.conv1 = nn.Conv2d(3, num_planes, kernel_size=3, padding=1, bias=False)\n","\n","        self.dense1 = self._make_dense_layers(block, num_planes, nblocks[0])\n","        num_planes += nblocks[0]*growth_rate\n","        out_planes = int(math.floor(num_planes*reduction))\n","        self.trans1 = Transition(num_planes, out_planes)\n","        num_planes = out_planes\n","\n","        self.dense2 = self._make_dense_layers(block, num_planes, nblocks[1])\n","        num_planes += nblocks[1]*growth_rate\n","        out_planes = int(math.floor(num_planes*reduction))\n","        self.trans2 = Transition(num_planes, out_planes)\n","        num_planes = out_planes\n","\n","        self.dense3 = self._make_dense_layers(block, num_planes, nblocks[2])\n","        num_planes += nblocks[2]*growth_rate\n","        out_planes = int(math.floor(num_planes*reduction))\n","        self.trans3 = Transition(num_planes, out_planes)\n","        num_planes = out_planes\n","\n","        self.dense4 = self._make_dense_layers(block, num_planes, nblocks[3])\n","        num_planes += nblocks[3]*growth_rate\n","\n","        self.bn = nn.BatchNorm2d(num_planes)\n","        self.linear = nn.Linear(num_planes, num_classes)\n","\n","    def _make_dense_layers(self, block, in_planes, nblock):\n","        layers = []\n","        for i in range(nblock):\n","            layers.append(block(in_planes, self.growth_rate))\n","            in_planes += self.growth_rate\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = self.conv1(x)\n","        out = self.trans1(self.dense1(out))\n","        out = self.trans2(self.dense2(out))\n","        out = self.trans3(self.dense3(out))\n","        out = self.dense4(out)\n","        out = F.avg_pool2d(F.relu(self.bn(out)), 4)\n","        out = out.view(out.size(0), -1)\n","        out = self.linear(out)\n","        return out\n","\n","def DenseNet121():\n","    return DenseNet(Bottleneck, [6,12,24,16], growth_rate=32)\n","\n","def DenseNet169():\n","    return DenseNet(Bottleneck, [6,12,32,32], growth_rate=32)\n","\n","def DenseNet201():\n","    return DenseNet(Bottleneck, [6,12,48,32], growth_rate=32)\n","\n","def DenseNet161():\n","    return DenseNet(Bottleneck, [6,12,36,24], growth_rate=48)\n","\n","def densenet_cifar():\n","    return DenseNet(Bottleneck, [6,12,24,16], growth_rate=12)\n","\n","def test():\n","    net = densenet_cifar()\n","    x = torch.randn(1,3,32,32)\n","    y = net(x)\n","    print(y)\n","\n","test()"],"execution_count":7,"outputs":[{"output_type":"stream","text":["tensor([[-0.2142, -0.2444,  0.0579,  0.2265,  0.0856,  0.1146, -0.0348,  0.2239,\n","          0.0715,  0.3321]], grad_fn=<AddmmBackward>)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KLi2bQhWvQT3","colab_type":"code","colab":{}},"source":["'''Dual Path Networks in PyTorch.'''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class Bottleneck(nn.Module):\n","    def __init__(self, last_planes, in_planes, out_planes, dense_depth, stride, first_layer):\n","        super(Bottleneck, self).__init__()\n","        self.out_planes = out_planes\n","        self.dense_depth = dense_depth\n","\n","        self.conv1 = nn.Conv2d(last_planes, in_planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv2 = nn.Conv2d(in_planes, in_planes, kernel_size=3, stride=stride, padding=1, groups=32, bias=False)\n","        self.bn2 = nn.BatchNorm2d(in_planes)\n","        self.conv3 = nn.Conv2d(in_planes, out_planes+dense_depth, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(out_planes+dense_depth)\n","\n","        self.shortcut = nn.Sequential()\n","        if first_layer:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(last_planes, out_planes+dense_depth, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(out_planes+dense_depth)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        x = self.shortcut(x)\n","        d = self.out_planes\n","        out = torch.cat([x[:,:d,:,:]+out[:,:d,:,:], x[:,d:,:,:], out[:,d:,:,:]], 1)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class DPN(nn.Module):\n","    def __init__(self, cfg):\n","        super(DPN, self).__init__()\n","        in_planes, out_planes = cfg['in_planes'], cfg['out_planes']\n","        num_blocks, dense_depth = cfg['num_blocks'], cfg['dense_depth']\n","\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.last_planes = 64\n","        self.layer1 = self._make_layer(in_planes[0], out_planes[0], num_blocks[0], dense_depth[0], stride=1)\n","        self.layer2 = self._make_layer(in_planes[1], out_planes[1], num_blocks[1], dense_depth[1], stride=2)\n","        self.layer3 = self._make_layer(in_planes[2], out_planes[2], num_blocks[2], dense_depth[2], stride=2)\n","        self.layer4 = self._make_layer(in_planes[3], out_planes[3], num_blocks[3], dense_depth[3], stride=2)\n","        self.linear = nn.Linear(out_planes[3]+(num_blocks[3]+1)*dense_depth[3], 10)\n","\n","    def _make_layer(self, in_planes, out_planes, num_blocks, dense_depth, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for i,stride in enumerate(strides):\n","            layers.append(Bottleneck(self.last_planes, in_planes, out_planes, dense_depth, stride, i==0))\n","            self.last_planes = out_planes + (i+2) * dense_depth\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.layer1(out)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = self.layer4(out)\n","        out = F.avg_pool2d(out, 4)\n","        out = out.view(out.size(0), -1)\n","        out = self.linear(out)\n","        return out\n","\n","\n","def DPN26():\n","    cfg = {\n","        'in_planes': (96,192,384,768),\n","        'out_planes': (256,512,1024,2048),\n","        'num_blocks': (2,2,2,2),\n","        'dense_depth': (16,32,24,128)\n","    }\n","    return DPN(cfg)\n","\n","def DPN92():\n","    cfg = {\n","        'in_planes': (96,192,384,768),\n","        'out_planes': (256,512,1024,2048),\n","        'num_blocks': (3,4,20,3),\n","        'dense_depth': (16,32,24,128)\n","    }\n","    return DPN(cfg)\n","\n","\n","def test():\n","    net = DPN92()\n","    x = torch.randn(1,3,32,32)\n","    y = net(x)\n","    print(y)\n","\n","# test()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FlizJ6sWvgY8","colab_type":"code","colab":{}},"source":["'''EfficientNet in PyTorch.\n","Paper: \"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks\".\n","'''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class Block(nn.Module):\n","    '''expand + depthwise + pointwise + squeeze-excitation'''\n","\n","    def __init__(self, in_planes, out_planes, expansion, stride):\n","        super(Block, self).__init__()\n","        self.stride = stride\n","\n","        planes = expansion * in_planes\n","        self.conv1 = nn.Conv2d(\n","            in_planes, planes, kernel_size=1, stride=1, padding=0, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n","                               stride=stride, padding=1, groups=planes, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(\n","            planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)\n","        self.bn3 = nn.BatchNorm2d(out_planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride == 1 and in_planes != out_planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, out_planes, kernel_size=1,\n","                          stride=1, padding=0, bias=False),\n","                nn.BatchNorm2d(out_planes),\n","            )\n","\n","        # SE layers\n","        self.fc1 = nn.Conv2d(out_planes, out_planes//16, kernel_size=1)\n","        self.fc2 = nn.Conv2d(out_planes//16, out_planes, kernel_size=1)\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        shortcut = self.shortcut(x) if self.stride == 1 else out\n","        # Squeeze-Excitation\n","        w = F.avg_pool2d(out, out.size(2))\n","        w = F.relu(self.fc1(w))\n","        w = self.fc2(w).sigmoid()\n","        out = out * w + shortcut\n","        return out\n","\n","\n","class EfficientNet(nn.Module):\n","    def __init__(self, cfg, num_classes=10):\n","        super(EfficientNet, self).__init__()\n","        self.cfg = cfg\n","        self.conv1 = nn.Conv2d(3, 32, kernel_size=3,\n","                               stride=1, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(32)\n","        self.layers = self._make_layers(in_planes=32)\n","        self.linear = nn.Linear(cfg[-1][1], num_classes)\n","\n","    def _make_layers(self, in_planes):\n","        layers = []\n","        for expansion, out_planes, num_blocks, stride in self.cfg:\n","            strides = [stride] + [1]*(num_blocks-1)\n","            for stride in strides:\n","                layers.append(Block(in_planes, out_planes, expansion, stride))\n","                in_planes = out_planes\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.layers(out)\n","        out = out.view(out.size(0), -1)\n","        out = self.linear(out)\n","        return out\n","\n","\n","def EfficientNetB0():\n","    # (expansion, out_planes, num_blocks, stride)\n","    cfg = [(1,  16, 1, 2),\n","           (6,  24, 2, 1),\n","           (6,  40, 2, 2),\n","           (6,  80, 3, 2),\n","           (6, 112, 3, 1),\n","           (6, 192, 4, 2),\n","           (6, 320, 1, 2)]\n","    return EfficientNet(cfg)\n","\n","\n","def test():\n","    net = EfficientNetB0()\n","    x = torch.randn(2, 3, 32, 32)\n","    y = net(x)\n","    print(y.shape)\n","\n","\n","# test()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4tttmO1avnX9","colab_type":"code","colab":{}},"source":["'''GoogLeNet with PyTorch.'''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class Inception(nn.Module):\n","    def __init__(self, in_planes, n1x1, n3x3red, n3x3, n5x5red, n5x5, pool_planes):\n","        super(Inception, self).__init__()\n","        # 1x1 conv branch\n","        self.b1 = nn.Sequential(\n","            nn.Conv2d(in_planes, n1x1, kernel_size=1),\n","            nn.BatchNorm2d(n1x1),\n","            nn.ReLU(True),\n","        )\n","\n","        # 1x1 conv -> 3x3 conv branch\n","        self.b2 = nn.Sequential(\n","            nn.Conv2d(in_planes, n3x3red, kernel_size=1),\n","            nn.BatchNorm2d(n3x3red),\n","            nn.ReLU(True),\n","            nn.Conv2d(n3x3red, n3x3, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(n3x3),\n","            nn.ReLU(True),\n","        )\n","\n","        # 1x1 conv -> 5x5 conv branch\n","        self.b3 = nn.Sequential(\n","            nn.Conv2d(in_planes, n5x5red, kernel_size=1),\n","            nn.BatchNorm2d(n5x5red),\n","            nn.ReLU(True),\n","            nn.Conv2d(n5x5red, n5x5, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(n5x5),\n","            nn.ReLU(True),\n","            nn.Conv2d(n5x5, n5x5, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(n5x5),\n","            nn.ReLU(True),\n","        )\n","\n","        # 3x3 pool -> 1x1 conv branch\n","        self.b4 = nn.Sequential(\n","            nn.MaxPool2d(3, stride=1, padding=1),\n","            nn.Conv2d(in_planes, pool_planes, kernel_size=1),\n","            nn.BatchNorm2d(pool_planes),\n","            nn.ReLU(True),\n","        )\n","\n","    def forward(self, x):\n","        y1 = self.b1(x)\n","        y2 = self.b2(x)\n","        y3 = self.b3(x)\n","        y4 = self.b4(x)\n","        return torch.cat([y1,y2,y3,y4], 1)\n","\n","\n","class GoogLeNet(nn.Module):\n","    def __init__(self):\n","        super(GoogLeNet, self).__init__()\n","        self.pre_layers = nn.Sequential(\n","            nn.Conv2d(3, 192, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(192),\n","            nn.ReLU(True),\n","        )\n","\n","        self.a3 = Inception(192,  64,  96, 128, 16, 32, 32)\n","        self.b3 = Inception(256, 128, 128, 192, 32, 96, 64)\n","\n","        self.maxpool = nn.MaxPool2d(3, stride=2, padding=1)\n","\n","        self.a4 = Inception(480, 192,  96, 208, 16,  48,  64)\n","        self.b4 = Inception(512, 160, 112, 224, 24,  64,  64)\n","        self.c4 = Inception(512, 128, 128, 256, 24,  64,  64)\n","        self.d4 = Inception(512, 112, 144, 288, 32,  64,  64)\n","        self.e4 = Inception(528, 256, 160, 320, 32, 128, 128)\n","\n","        self.a5 = Inception(832, 256, 160, 320, 32, 128, 128)\n","        self.b5 = Inception(832, 384, 192, 384, 48, 128, 128)\n","\n","        self.avgpool = nn.AvgPool2d(8, stride=1)\n","        self.linear = nn.Linear(1024, 10)\n","\n","    def forward(self, x):\n","        out = self.pre_layers(x)\n","        out = self.a3(out)\n","        out = self.b3(out)\n","        out = self.maxpool(out)\n","        out = self.a4(out)\n","        out = self.b4(out)\n","        out = self.c4(out)\n","        out = self.d4(out)\n","        out = self.e4(out)\n","        out = self.maxpool(out)\n","        out = self.a5(out)\n","        out = self.b5(out)\n","        out = self.avgpool(out)\n","        out = out.view(out.size(0), -1)\n","        out = self.linear(out)\n","        return out\n","\n","\n","def test():\n","    net = GoogLeNet()\n","    x = torch.randn(1,3,32,32)\n","    y = net(x)\n","    print(y.size())\n","\n","# test()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7JTS-B13v0JE","colab_type":"code","colab":{}},"source":["'''MobileNet in PyTorch.\n","See the paper \"MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications\"\n","for more details.\n","'''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class Block(nn.Module):\n","    '''Depthwise conv + Pointwise conv'''\n","    def __init__(self, in_planes, out_planes, stride=1):\n","        super(Block, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, in_planes, kernel_size=3, stride=stride, padding=1, groups=in_planes, bias=False)\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv2 = nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)\n","        self.bn2 = nn.BatchNorm2d(out_planes)\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        return out\n","\n","\n","class MobileNet(nn.Module):\n","    # (128,2) means conv planes=128, conv stride=2, by default conv stride=1\n","    cfg = [64, (128,2), 128, (256,2), 256, (512,2), 512, 512, 512, 512, 512, (1024,2), 1024]\n","\n","    def __init__(self, num_classes=10):\n","        super(MobileNet, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(32)\n","        self.layers = self._make_layers(in_planes=32)\n","        self.linear = nn.Linear(1024, num_classes)\n","\n","    def _make_layers(self, in_planes):\n","        layers = []\n","        for x in self.cfg:\n","            out_planes = x if isinstance(x, int) else x[0]\n","            stride = 1 if isinstance(x, int) else x[1]\n","            layers.append(Block(in_planes, out_planes, stride))\n","            in_planes = out_planes\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.layers(out)\n","        out = F.avg_pool2d(out, 2)\n","        out = out.view(out.size(0), -1)\n","        out = self.linear(out)\n","        return out\n","\n","\n","def test():\n","    net = MobileNet()\n","    x = torch.randn(1,3,32,32)\n","    y = net(x)\n","    print(y.size())\n","\n","# test()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DZNieGqSv0y0","colab_type":"code","colab":{}},"source":["'''MobileNetV2 in PyTorch.\n","See the paper \"Inverted Residuals and Linear Bottlenecks:\n","Mobile Networks for Classification, Detection and Segmentation\" for more details.\n","'''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class Block(nn.Module):\n","    '''expand + depthwise + pointwise'''\n","    def __init__(self, in_planes, out_planes, expansion, stride):\n","        super(Block, self).__init__()\n","        self.stride = stride\n","\n","        planes = expansion * in_planes\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, stride=1, padding=0, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, groups=planes, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)\n","        self.bn3 = nn.BatchNorm2d(out_planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride == 1 and in_planes != out_planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False),\n","                nn.BatchNorm2d(out_planes),\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out = out + self.shortcut(x) if self.stride==1 else out\n","        return out\n","\n","\n","class MobileNetV2(nn.Module):\n","    # (expansion, out_planes, num_blocks, stride)\n","    cfg = [(1,  16, 1, 1),\n","           (6,  24, 2, 1),  # NOTE: change stride 2 -> 1 for CIFAR10\n","           (6,  32, 3, 2),\n","           (6,  64, 4, 2),\n","           (6,  96, 3, 1),\n","           (6, 160, 3, 2),\n","           (6, 320, 1, 1)]\n","\n","    def __init__(self, num_classes=10):\n","        super(MobileNetV2, self).__init__()\n","        # NOTE: change conv1 stride 2 -> 1 for CIFAR10\n","        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(32)\n","        self.layers = self._make_layers(in_planes=32)\n","        self.conv2 = nn.Conv2d(320, 1280, kernel_size=1, stride=1, padding=0, bias=False)\n","        self.bn2 = nn.BatchNorm2d(1280)\n","        self.linear = nn.Linear(1280, num_classes)\n","\n","    def _make_layers(self, in_planes):\n","        layers = []\n","        for expansion, out_planes, num_blocks, stride in self.cfg:\n","            strides = [stride] + [1]*(num_blocks-1)\n","            for stride in strides:\n","                layers.append(Block(in_planes, out_planes, expansion, stride))\n","                in_planes = out_planes\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.layers(out)\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        # NOTE: change pooling kernel_size 7 -> 4 for CIFAR10\n","        out = F.avg_pool2d(out, 4)\n","        out = out.view(out.size(0), -1)\n","        out = self.linear(out)\n","        return out\n","\n","\n","def test():\n","    net = MobileNetV2()\n","    x = torch.randn(2,3,32,32)\n","    y = net(x)\n","    print(y.size())\n","\n","# test()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EN0UaMvBv9hc","colab_type":"code","colab":{}},"source":["'''PNASNet in PyTorch.\n","Paper: Progressive Neural Architecture Search\n","'''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class SepConv(nn.Module):\n","    '''Separable Convolution.'''\n","    def __init__(self, in_planes, out_planes, kernel_size, stride):\n","        super(SepConv, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, out_planes,\n","                               kernel_size, stride,\n","                               padding=(kernel_size-1)//2,\n","                               bias=False, groups=in_planes)\n","        self.bn1 = nn.BatchNorm2d(out_planes)\n","\n","    def forward(self, x):\n","        return self.bn1(self.conv1(x))\n","\n","\n","class CellA(nn.Module):\n","    def __init__(self, in_planes, out_planes, stride=1):\n","        super(CellA, self).__init__()\n","        self.stride = stride\n","        self.sep_conv1 = SepConv(in_planes, out_planes, kernel_size=7, stride=stride)\n","        if stride==2:\n","            self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)\n","            self.bn1 = nn.BatchNorm2d(out_planes)\n","\n","    def forward(self, x):\n","        y1 = self.sep_conv1(x)\n","        y2 = F.max_pool2d(x, kernel_size=3, stride=self.stride, padding=1)\n","        if self.stride==2:\n","            y2 = self.bn1(self.conv1(y2))\n","        return F.relu(y1+y2)\n","\n","class CellB(nn.Module):\n","    def __init__(self, in_planes, out_planes, stride=1):\n","        super(CellB, self).__init__()\n","        self.stride = stride\n","        # Left branch\n","        self.sep_conv1 = SepConv(in_planes, out_planes, kernel_size=7, stride=stride)\n","        self.sep_conv2 = SepConv(in_planes, out_planes, kernel_size=3, stride=stride)\n","        # Right branch\n","        self.sep_conv3 = SepConv(in_planes, out_planes, kernel_size=5, stride=stride)\n","        if stride==2:\n","            self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)\n","            self.bn1 = nn.BatchNorm2d(out_planes)\n","        # Reduce channels\n","        self.conv2 = nn.Conv2d(2*out_planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)\n","        self.bn2 = nn.BatchNorm2d(out_planes)\n","\n","    def forward(self, x):\n","        # Left branch\n","        y1 = self.sep_conv1(x)\n","        y2 = self.sep_conv2(x)\n","        # Right branch\n","        y3 = F.max_pool2d(x, kernel_size=3, stride=self.stride, padding=1)\n","        if self.stride==2:\n","            y3 = self.bn1(self.conv1(y3))\n","        y4 = self.sep_conv3(x)\n","        # Concat & reduce channels\n","        b1 = F.relu(y1+y2)\n","        b2 = F.relu(y3+y4)\n","        y = torch.cat([b1,b2], 1)\n","        return F.relu(self.bn2(self.conv2(y)))\n","\n","class PNASNet(nn.Module):\n","    def __init__(self, cell_type, num_cells, num_planes):\n","        super(PNASNet, self).__init__()\n","        self.in_planes = num_planes\n","        self.cell_type = cell_type\n","\n","        self.conv1 = nn.Conv2d(3, num_planes, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(num_planes)\n","\n","        self.layer1 = self._make_layer(num_planes, num_cells=6)\n","        self.layer2 = self._downsample(num_planes*2)\n","        self.layer3 = self._make_layer(num_planes*2, num_cells=6)\n","        self.layer4 = self._downsample(num_planes*4)\n","        self.layer5 = self._make_layer(num_planes*4, num_cells=6)\n","\n","        self.linear = nn.Linear(num_planes*4, 10)\n","\n","    def _make_layer(self, planes, num_cells):\n","        layers = []\n","        for _ in range(num_cells):\n","            layers.append(self.cell_type(self.in_planes, planes, stride=1))\n","            self.in_planes = planes\n","        return nn.Sequential(*layers)\n","\n","    def _downsample(self, planes):\n","        layer = self.cell_type(self.in_planes, planes, stride=2)\n","        self.in_planes = planes\n","        return layer\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.layer1(out)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = self.layer4(out)\n","        out = self.layer5(out)\n","        out = F.avg_pool2d(out, 8)\n","        out = self.linear(out.view(out.size(0), -1))\n","        return out\n","\n","\n","def PNASNetA():\n","    return PNASNet(CellA, num_cells=6, num_planes=44)\n","\n","def PNASNetB():\n","    return PNASNet(CellB, num_cells=6, num_planes=32)\n","\n","\n","def test():\n","    net = PNASNetB()\n","    x = torch.randn(1,3,32,32)\n","    y = net(x)\n","    print(y)\n","\n","# test()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jMcfW-sqwEfD","colab_type":"code","colab":{}},"source":["'''Pre-activation ResNet in PyTorch.\n","Reference:\n","[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Identity Mappings in Deep Residual Networks. arXiv:1603.05027\n","'''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class PreActBlock(nn.Module):\n","    '''Pre-activation version of the BasicBlock.'''\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBlock, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n","\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class PreActBottleneck(nn.Module):\n","    '''Pre-activation version of the original Bottleneck module.'''\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBottleneck, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out = self.conv3(F.relu(self.bn3(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class PreActResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=10):\n","        super(PreActResNet, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.linear = nn.Linear(512*block.expansion, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = self.conv1(x)\n","        out = self.layer1(out)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = self.layer4(out)\n","        out = F.avg_pool2d(out, 4)\n","        out = out.view(out.size(0), -1)\n","        out = self.linear(out)\n","        return out\n","\n","\n","def PreActResNet18():\n","    return PreActResNet(PreActBlock, [2,2,2,2])\n","\n","def PreActResNet34():\n","    return PreActResNet(PreActBlock, [3,4,6,3])\n","\n","def PreActResNet50():\n","    return PreActResNet(PreActBottleneck, [3,4,6,3])\n","\n","def PreActResNet101():\n","    return PreActResNet(PreActBottleneck, [3,4,23,3])\n","\n","def PreActResNet152():\n","    return PreActResNet(PreActBottleneck, [3,8,36,3])\n","\n","\n","def test():\n","    net = PreActResNet18()\n","    y = net((torch.randn(1,3,32,32)))\n","    print(y.size())\n","\n","# test()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gf78EBegwLBT","colab_type":"code","colab":{}},"source":["'''ResNet in PyTorch.\n","For Pre-activation ResNet, see 'preact_resnet.py'.\n","Reference:\n","[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n","'''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=10):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.linear = nn.Linear(512*block.expansion, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.layer1(out)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = self.layer4(out)\n","        out = F.avg_pool2d(out, 4)\n","        out = out.view(out.size(0), -1)\n","        out = self.linear(out)\n","        return out\n","\n","\n","def ResNet18():\n","    return ResNet(BasicBlock, [2,2,2,2])\n","\n","def ResNet34():\n","    return ResNet(BasicBlock, [3,4,6,3])\n","\n","def ResNet50():\n","    return ResNet(Bottleneck, [3,4,6,3])\n","\n","def ResNet101():\n","    return ResNet(Bottleneck, [3,4,23,3])\n","\n","def ResNet152():\n","    return ResNet(Bottleneck, [3,8,36,3])\n","\n","\n","def test():\n","    net = ResNet18()\n","    y = net(torch.randn(1,3,32,32))\n","    print(y.size())\n","\n","# test()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0TSO9qZzwWOz","colab_type":"code","colab":{}},"source":["'''ResNeXt in PyTorch.\n","See the paper \"Aggregated Residual Transformations for Deep Neural Networks\" for more details.\n","'''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class Block(nn.Module):\n","    '''Grouped convolution block.'''\n","    expansion = 2\n","\n","    def __init__(self, in_planes, cardinality=32, bottleneck_width=4, stride=1):\n","        super(Block, self).__init__()\n","        group_width = cardinality * bottleneck_width\n","        self.conv1 = nn.Conv2d(in_planes, group_width, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(group_width)\n","        self.conv2 = nn.Conv2d(group_width, group_width, kernel_size=3, stride=stride, padding=1, groups=cardinality, bias=False)\n","        self.bn2 = nn.BatchNorm2d(group_width)\n","        self.conv3 = nn.Conv2d(group_width, self.expansion*group_width, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.expansion*group_width)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*group_width:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*group_width, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*group_width)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class ResNeXt(nn.Module):\n","    def __init__(self, num_blocks, cardinality, bottleneck_width, num_classes=10):\n","        super(ResNeXt, self).__init__()\n","        self.cardinality = cardinality\n","        self.bottleneck_width = bottleneck_width\n","        self.in_planes = 64\n","\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(num_blocks[0], 1)\n","        self.layer2 = self._make_layer(num_blocks[1], 2)\n","        self.layer3 = self._make_layer(num_blocks[2], 2)\n","        # self.layer4 = self._make_layer(num_blocks[3], 2)\n","        self.linear = nn.Linear(cardinality*bottleneck_width*8, num_classes)\n","\n","    def _make_layer(self, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(Block(self.in_planes, self.cardinality, self.bottleneck_width, stride))\n","            self.in_planes = Block.expansion * self.cardinality * self.bottleneck_width\n","        # Increase bottleneck_width by 2 after each stage.\n","        self.bottleneck_width *= 2\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.layer1(out)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        # out = self.layer4(out)\n","        out = F.avg_pool2d(out, 8)\n","        out = out.view(out.size(0), -1)\n","        out = self.linear(out)\n","        return out\n","\n","\n","def ResNeXt29_2x64d():\n","    return ResNeXt(num_blocks=[3,3,3], cardinality=2, bottleneck_width=64)\n","\n","def ResNeXt29_4x64d():\n","    return ResNeXt(num_blocks=[3,3,3], cardinality=4, bottleneck_width=64)\n","\n","def ResNeXt29_8x64d():\n","    return ResNeXt(num_blocks=[3,3,3], cardinality=8, bottleneck_width=64)\n","\n","def ResNeXt29_32x4d():\n","    return ResNeXt(num_blocks=[3,3,3], cardinality=32, bottleneck_width=4)\n","\n","def test_resnext():\n","    net = ResNeXt29_2x64d()\n","    x = torch.randn(1,3,32,32)\n","    y = net(x)\n","    print(y.size())\n","\n","# test_resnext()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WG-ycvGIweuL","colab_type":"code","colab":{}},"source":["'''SENet in PyTorch.\n","SENet is the winner of ImageNet-2017. The paper is not released yet.\n","'''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class BasicBlock(nn.Module):\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(planes)\n","            )\n","\n","        # SE layers\n","        self.fc1 = nn.Conv2d(planes, planes//16, kernel_size=1)  # Use nn.Conv2d instead of nn.Linear\n","        self.fc2 = nn.Conv2d(planes//16, planes, kernel_size=1)\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","\n","        # Squeeze\n","        w = F.avg_pool2d(out, out.size(2))\n","        w = F.relu(self.fc1(w))\n","        w = F.sigmoid(self.fc2(w))\n","        # Excitation\n","        out = out * w  # New broadcasting feature from v0.2!\n","\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBlock(nn.Module):\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBlock, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n","\n","        if stride != 1 or in_planes != planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","        # SE layers\n","        self.fc1 = nn.Conv2d(planes, planes//16, kernel_size=1)\n","        self.fc2 = nn.Conv2d(planes//16, planes, kernel_size=1)\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","\n","        # Squeeze\n","        w = F.avg_pool2d(out, out.size(2))\n","        w = F.relu(self.fc1(w))\n","        w = F.sigmoid(self.fc2(w))\n","        # Excitation\n","        out = out * w\n","\n","        out += shortcut\n","        return out\n","\n","\n","class SENet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=10):\n","        super(SENet, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block,  64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.linear = nn.Linear(512, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.layer1(out)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = self.layer4(out)\n","        out = F.avg_pool2d(out, 4)\n","        out = out.view(out.size(0), -1)\n","        out = self.linear(out)\n","        return out\n","\n","\n","def SENet18():\n","    return SENet(PreActBlock, [2,2,2,2])\n","\n","\n","def test():\n","    net = SENet18()\n","    y = net(torch.randn(1,3,32,32))\n","    print(y.size())\n","\n","# test()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OfOsf0Jswnha","colab_type":"code","colab":{}},"source":["\n","'''ShuffleNet in PyTorch.\n","See the paper \"ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices\" for more details.\n","'''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class ShuffleBlock(nn.Module):\n","    def __init__(self, groups):\n","        super(ShuffleBlock, self).__init__()\n","        self.groups = groups\n","\n","    def forward(self, x):\n","        '''Channel shuffle: [N,C,H,W] -> [N,g,C/g,H,W] -> [N,C/g,g,H,w] -> [N,C,H,W]'''\n","        N,C,H,W = x.size()\n","        g = self.groups\n","        return x.view(N,g,C//g,H,W).permute(0,2,1,3,4).reshape(N,C,H,W)\n","\n","\n","class Bottleneck(nn.Module):\n","    def __init__(self, in_planes, out_planes, stride, groups):\n","        super(Bottleneck, self).__init__()\n","        self.stride = stride\n","\n","        mid_planes = out_planes/4\n","        g = 1 if in_planes==24 else groups\n","        self.conv1 = nn.Conv2d(in_planes, mid_planes, kernel_size=1, groups=g, bias=False)\n","        self.bn1 = nn.BatchNorm2d(mid_planes)\n","        self.shuffle1 = ShuffleBlock(groups=g)\n","        self.conv2 = nn.Conv2d(mid_planes, mid_planes, kernel_size=3, stride=stride, padding=1, groups=mid_planes, bias=False)\n","        self.bn2 = nn.BatchNorm2d(mid_planes)\n","        self.conv3 = nn.Conv2d(mid_planes, out_planes, kernel_size=1, groups=groups, bias=False)\n","        self.bn3 = nn.BatchNorm2d(out_planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride == 2:\n","            self.shortcut = nn.Sequential(nn.AvgPool2d(3, stride=2, padding=1))\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.shuffle1(out)\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        res = self.shortcut(x)\n","        out = F.relu(torch.cat([out,res], 1)) if self.stride==2 else F.relu(out+res)\n","        return out\n","\n","\n","class ShuffleNet(nn.Module):\n","    def __init__(self, cfg):\n","        super(ShuffleNet, self).__init__()\n","        out_planes = cfg['out_planes']\n","        num_blocks = cfg['num_blocks']\n","        groups = cfg['groups']\n","\n","        self.conv1 = nn.Conv2d(3, 24, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(24)\n","        self.in_planes = 24\n","        self.layer1 = self._make_layer(out_planes[0], num_blocks[0], groups)\n","        self.layer2 = self._make_layer(out_planes[1], num_blocks[1], groups)\n","        self.layer3 = self._make_layer(out_planes[2], num_blocks[2], groups)\n","        self.linear = nn.Linear(out_planes[2], 10)\n","\n","    def _make_layer(self, out_planes, num_blocks, groups):\n","        layers = []\n","        for i in range(num_blocks):\n","            stride = 2 if i == 0 else 1\n","            cat_planes = self.in_planes if i == 0 else 0\n","            layers.append(Bottleneck(self.in_planes, out_planes-cat_planes, stride=stride, groups=groups))\n","            self.in_planes = out_planes\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.layer1(out)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = F.avg_pool2d(out, 4)\n","        out = out.view(out.size(0), -1)\n","        out = self.linear(out)\n","        return out\n","\n","\n","def ShuffleNetG2():\n","    cfg = {\n","        'out_planes': [200,400,800],\n","        'num_blocks': [4,8,4],\n","        'groups': 2\n","    }\n","    return ShuffleNet(cfg)\n","\n","def ShuffleNetG3():\n","    cfg = {\n","        'out_planes': [240,480,960],\n","        'num_blocks': [4,8,4],\n","        'groups': 3\n","    }\n","    return ShuffleNet(cfg)\n","\n","\n","def test():\n","    net = ShuffleNetG2()\n","    x = torch.randn(1,3,32,32)\n","    y = net(x)\n","    print(y)\n","\n","# test()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"q7fod298w1fS","colab_type":"code","colab":{}},"source":["'''ShuffleNetV2 in PyTorch.\n","See the paper \"ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design\" for more details.\n","'''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class ShuffleBlock(nn.Module):\n","    def __init__(self, groups=2):\n","        super(ShuffleBlock, self).__init__()\n","        self.groups = groups\n","\n","    def forward(self, x):\n","        '''Channel shuffle: [N,C,H,W] -> [N,g,C/g,H,W] -> [N,C/g,g,H,w] -> [N,C,H,W]'''\n","        N, C, H, W = x.size()\n","        g = self.groups\n","        return x.view(N, g, C//g, H, W).permute(0, 2, 1, 3, 4).reshape(N, C, H, W)\n","\n","\n","class SplitBlock(nn.Module):\n","    def __init__(self, ratio):\n","        super(SplitBlock, self).__init__()\n","        self.ratio = ratio\n","\n","    def forward(self, x):\n","        c = int(x.size(1) * self.ratio)\n","        return x[:, :c, :, :], x[:, c:, :, :]\n","\n","\n","class BasicBlock(nn.Module):\n","    def __init__(self, in_channels, split_ratio=0.5):\n","        super(BasicBlock, self).__init__()\n","        self.split = SplitBlock(split_ratio)\n","        in_channels = int(in_channels * split_ratio)\n","        self.conv1 = nn.Conv2d(in_channels, in_channels,\n","                               kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(in_channels)\n","        self.conv2 = nn.Conv2d(in_channels, in_channels,\n","                               kernel_size=3, stride=1, padding=1, groups=in_channels, bias=False)\n","        self.bn2 = nn.BatchNorm2d(in_channels)\n","        self.conv3 = nn.Conv2d(in_channels, in_channels,\n","                               kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(in_channels)\n","        self.shuffle = ShuffleBlock()\n","\n","    def forward(self, x):\n","        x1, x2 = self.split(x)\n","        out = F.relu(self.bn1(self.conv1(x2)))\n","        out = self.bn2(self.conv2(out))\n","        out = F.relu(self.bn3(self.conv3(out)))\n","        out = torch.cat([x1, out], 1)\n","        out = self.shuffle(out)\n","        return out\n","\n","\n","class DownBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(DownBlock, self).__init__()\n","        mid_channels = out_channels // 2\n","        # left\n","        self.conv1 = nn.Conv2d(in_channels, in_channels,\n","                               kernel_size=3, stride=2, padding=1, groups=in_channels, bias=False)\n","        self.bn1 = nn.BatchNorm2d(in_channels)\n","        self.conv2 = nn.Conv2d(in_channels, mid_channels,\n","                               kernel_size=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(mid_channels)\n","        # right\n","        self.conv3 = nn.Conv2d(in_channels, mid_channels,\n","                               kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(mid_channels)\n","        self.conv4 = nn.Conv2d(mid_channels, mid_channels,\n","                               kernel_size=3, stride=2, padding=1, groups=mid_channels, bias=False)\n","        self.bn4 = nn.BatchNorm2d(mid_channels)\n","        self.conv5 = nn.Conv2d(mid_channels, mid_channels,\n","                               kernel_size=1, bias=False)\n","        self.bn5 = nn.BatchNorm2d(mid_channels)\n","\n","        self.shuffle = ShuffleBlock()\n","\n","    def forward(self, x):\n","        # left\n","        out1 = self.bn1(self.conv1(x))\n","        out1 = F.relu(self.bn2(self.conv2(out1)))\n","        # right\n","        out2 = F.relu(self.bn3(self.conv3(x)))\n","        out2 = self.bn4(self.conv4(out2))\n","        out2 = F.relu(self.bn5(self.conv5(out2)))\n","        # concat\n","        out = torch.cat([out1, out2], 1)\n","        out = self.shuffle(out)\n","        return out\n","\n","\n","class ShuffleNetV2(nn.Module):\n","    def __init__(self, net_size):\n","        super(ShuffleNetV2, self).__init__()\n","        out_channels = configs[net_size]['out_channels']\n","        num_blocks = configs[net_size]['num_blocks']\n","\n","        self.conv1 = nn.Conv2d(3, 24, kernel_size=3,\n","                               stride=1, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(24)\n","        self.in_channels = 24\n","        self.layer1 = self._make_layer(out_channels[0], num_blocks[0])\n","        self.layer2 = self._make_layer(out_channels[1], num_blocks[1])\n","        self.layer3 = self._make_layer(out_channels[2], num_blocks[2])\n","        self.conv2 = nn.Conv2d(out_channels[2], out_channels[3],\n","                               kernel_size=1, stride=1, padding=0, bias=False)\n","        self.bn2 = nn.BatchNorm2d(out_channels[3])\n","        self.linear = nn.Linear(out_channels[3], 10)\n","\n","    def _make_layer(self, out_channels, num_blocks):\n","        layers = [DownBlock(self.in_channels, out_channels)]\n","        for i in range(num_blocks):\n","            layers.append(BasicBlock(out_channels))\n","            self.in_channels = out_channels\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        # out = F.max_pool2d(out, 3, stride=2, padding=1)\n","        out = self.layer1(out)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = F.avg_pool2d(out, 4)\n","        out = out.view(out.size(0), -1)\n","        out = self.linear(out)\n","        return out\n","\n","\n","configs = {\n","    0.5: {\n","        'out_channels': (48, 96, 192, 1024),\n","        'num_blocks': (3, 7, 3)\n","    },\n","\n","    1: {\n","        'out_channels': (116, 232, 464, 1024),\n","        'num_blocks': (3, 7, 3)\n","    },\n","    1.5: {\n","        'out_channels': (176, 352, 704, 1024),\n","        'num_blocks': (3, 7, 3)\n","    },\n","    2: {\n","        'out_channels': (224, 488, 976, 2048),\n","        'num_blocks': (3, 7, 3)\n","    }\n","}\n","\n","\n","def test():\n","    net = ShuffleNetV2(net_size=0.5)\n","    x = torch.randn(3, 3, 32, 32)\n","    y = net(x)\n","    print(y.shape)\n","\n","\n","# test()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9gNL9iaQw6Ii","colab_type":"code","colab":{}},"source":["'''VGG11/13/16/19 in Pytorch.'''\n","import torch\n","import torch.nn as nn\n","\n","\n","cfg = {\n","    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n","    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n","    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n","    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n","}\n","\n","\n","class VGG(nn.Module):\n","    def __init__(self, vgg_name):\n","        super(VGG, self).__init__()\n","        self.features = self._make_layers(cfg[vgg_name])\n","        self.classifier = nn.Linear(512, 10)\n","\n","    def forward(self, x):\n","        out = self.features(x)\n","        out = out.view(out.size(0), -1)\n","        out = self.classifier(out)\n","        return out\n","\n","    def _make_layers(self, cfg):\n","        layers = []\n","        in_channels = 3\n","        for x in cfg:\n","            if x == 'M':\n","                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n","            else:\n","                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n","                           nn.BatchNorm2d(x),\n","                           nn.ReLU(inplace=True)]\n","                in_channels = x\n","        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n","        return nn.Sequential(*layers)\n","\n","\n","def test():\n","    net = VGG('VGG11')\n","    x = torch.randn(2,3,32,32)\n","    y = net(x)\n","    print(y.size())\n","\n","# test()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EJoAxd8oxB9S","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}